## 四、SOM分类

用SOM对三样目标进行分类，测试集，训练集各占一半。

### 1. 基础知识

自组织映射神经网络， 即Self Organizing Maps (SOM)， 可以对数据进行无监督学习聚类。它的思想很简单，本质上是一种只有输入层--隐藏层的神经网络。隐藏层中的一个节点代表一个需要聚成的类。训练时采用“竞争学习”的方式，每个输入的样例在隐藏层中找到一个和它最匹配的节点，称为它的激活节点，也叫“winning neuron”。紧接着用随机梯度下降法更新激活节点的参数。同时，和激活节点临近的点也根据它们距离激活节点的远近而适当地更新参数。

所以，SOM的一个特点是，隐藏层的节点是有拓扑关系的。这个拓扑关系需要我们确定，如果想要一维的模型，那么隐藏节点依次连成一条线；如果想要二维的拓扑关系，那么就行成一个平面，如下图所示（也叫Kohonen Network）。

![SOM结构](E:\17年学习\神经网络\神经网络作业二\NN_second_homework\SOM\报告用图\SOM结构.jpg)

既然隐藏层是有拓扑关系的，所以我们也可以说，SOM可以把任意维度的输入离散化到一维或者二维(更高维度的不常见)的离散空间上。Computation layer里面的节点与Input layer的节点是全连接的。

拓扑关系确定后，开始计算过程，大体分成几个部分：

1. 初始化：每个节点随机初始化自己的参数。每个节点的参数个数与Input的维度相同。

2. 对于每一个输入数据，找到与它最相配的节点。假设输入时D维的， 即 $X=\{x_i, i=1,...,D\}$，那么判别函数可以为欧几里得距离：

   ​
   $$
   d_j(x)=\sum_{i=1}^D（x_i-w_{ji}）^2
   $$

3. 找到激活节点I(x)之后，我们也希望更新和它临近的节点。令$S_{ij}$表示节点i和j之间的距离，对于$I(x)$临近的节点，分配给它们一个更新权重：
   $$
   T_{j,I(x)}=exp(-\frac{S_{j,I(x)}^2}{2\sigma^2})
   $$

4. 接着就是更新节点的参数,按照梯度下降法更新：
   $$
   \Delta w_{ji}=\eta(t)\cdot T_{j,I(x)}(t)\cdot(x_i-w_{ji})
   $$
   迭代直到收敛。

### 2. 仿真方法

#### 2.1 样本的选择

本次测试的样本来源于实验室之前采集的人像数据库，之前用于朝向检测算法的，原数据库中有4000多张图片，分人和朝向保存，此次将其作为人像数据库使用，按人分类。所有的图片都被预先处理为$64\times64$大小，图像格式为bmp格式，灰度图。

在实验过程中，选择三个人的图片各70张，35张作为测试集，35张作为训练集，同时为了加快运算速度，在送入分类前，进行了PCA提取主元。

读入BMP格式的图片后，在matlab中是一个$64\times64$的数组，每个位代表对应的像素值，大小从0-255不等，初始权值选择0-1区间内的随机数，为了让权值迭代能对图片特征神经元的选择产生作用，我们需要先对图片进行归一化，并且将矩阵改变为列向量。

### 3. 分类结果

#### 3.1 SOM分类结果

神经元数目取400个($20\times20$)，初始权值取随机数，邻域函数方差选为20，迭代步长设置为0.3，权值迭代次数为1000次。得到结果如下：

![SOM结果](E:\17年学习\神经网络\神经网络作业二\NN_second_homework\SOM\报告用图\SOM结果.png)

图中显示的是训练之后特征神经元的分布，每一种颜色的点代表某一种类。可以得到87.61%的测试正确率。运行时间为6.5738秒。可以看到结果的分布比较均匀，但0和1和2在下方有一些交叉，这些交叉可能是误差的来源。

#### 3.2 结果的影响因素

##### 3.2.1 网络大小

选择不同的神经元总数对于识别效果也有较大的影响，取平方数，[64,256,900,1600]，迭代步代设置为0.3，得到结果如下：

| 神经元数目 | 准确率    | 耗时/s   |
| ----- | ------ | ------ |
| 144   | 0.4952 | 3.4487 |
| 256   | 0.6190 | 4.2152 |
| 400   | 0.8762 | 6.5738 |
| 900   | 0.8857 | 11.16  |
| 1600  | 0.8857 | 17.11  |

(为了篇幅考虑，只放出两张图)

![SOM结果144](E:\17年学习\神经网络\神经网络作业二\NN_second_homework\SOM\报告用图\SOM结果144.png)



![SOM结果1600](E:\17年学习\神经网络\神经网络作业二\NN_second_homework\SOM\报告用图\SOM结果1600.png)

可以看到随着节点数增多，正确率是会上升的，但是存在一个极限，同时，节点数增多，会带另一个问题即是运行时间变长。这是因为神经元数量增多，每个样本占据的区域与其他样本区域的混叠较小。如果神经元分布比较离散（神经元数目较多），则识别率能上升，但是计算量更大了，所以计算时间也会增多。

#### 3.2.2 邻域函数参数影响：

仍然取神经元为400个，步幅为0.3，取方差为[1,3,5,10,20,40,60]。

得到结果如下：

| 方差   | 正确率     | 时间/s   |
| ---- | ------- | ------ |
| 1    | 0.81905 | 7.1986 |
| 3    | 0.80952 | 6.9959 |
| 5    | 0.9048  | 6.5827 |
| 10   | 0.85714 | 6.0542 |
| 20   | 0.87619 | 6.5738 |
| 40   | 0.89524 | 6.3178 |
| 60   | 0.78095 | 6.3285 |

方差为1：

![SOM结果1](E:\17年学习\神经网络\神经网络作业二\NN_second_homework\SOM\报告用图\SOM结果1.png)

方差为60：

![SOM结果60](E:\17年学习\神经网络\神经网络作业二\NN_second_homework\SOM\报告用图\SOM结果60.png)

可以看到，在方差取5-40左右，正确率都很高但过高或过低都为影响到正确率，这应该是由于方差影响的是邻域的大小，当方差过大时邻域也很大，会使各样本重叠增大，进而降低正确率。同时，这个对于运行时间的影响不大。

#### 3.2.3 邻域函数参数影响：

仍然取神经元为400个，方差为20，取步幅为[0.1,0.3,0.5,0.7,0.9]。

| 方差   | 正确率     | 时间/s   |
| ---- | ------- | ------ |
| 0.1  | 0.89524 | 6.29   |
| 0.3  | 0.87619 | 6.5738 |
| 0.5  | 0.66667 | 6.3716 |
| 0.7  | 0.8857  | 6.2649 |
| 0.9  | 0.8381  | 6.1592 |

![SOM结果0.1](E:\17年学习\神经网络\神经网络作业二\NN_second_homework\SOM\报告用图\SOM结果0.1.png)

![SOM结果0.9](E:\17年学习\神经网络\神经网络作业二\NN_second_homework\SOM\报告用图\SOM结果0.9.png)

可以看到在步长小的时候对结果影响不大，当步长过大，结果会不收敛。



### 3. 总结

综上，取400个神经元，步幅为0.3，方差为5时达到最优准确率，为0.9048。

SOM具有一定的聚类效果和可视化效果，对于人像分类有很好的效果。对于结果，神经元的个数增大，步幅减小，取合适的方差可以取得更好的效果。但是由于初始权值是随机的，因此结果不是完全稳定的。

时间方面，SOM识别的速度主要与迭代次数，神经元数目有关，更多的神经元和更多的迭代次数能增强识别率，但是会增加计算时间，需要根据实际需要均衡两者。







